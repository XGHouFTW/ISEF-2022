{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XGHouFTW/py-cryptopredict/blob/main/LSTM_Updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gS2OkcDNB6Ds",
        "outputId": "91fc4699-d49c-4629-ad01-17048e34e6c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/MyDrive; to attempt to forcibly remount, call drive.mount(\"/content/MyDrive\", force_remount=True).\n",
            "                   Date  BTC-ChangeFactor  ETH-ChangeFactor  \\\n",
            "0    2020-01-31 0:00:00          0.983406          0.975226   \n",
            "1    2020-02-01 0:00:00          1.004977          1.019766   \n",
            "2    2020-02-02 0:00:00          0.995159          1.027706   \n",
            "3    2020-02-03 0:00:00          0.994525          1.006668   \n",
            "4    2020-02-04 0:00:00          0.987961          0.996781   \n",
            "..                  ...               ...               ...   \n",
            "727  2022-01-27 0:00:00          1.008044          0.982090   \n",
            "728  2022-01-28 0:00:00          1.017665          1.051802   \n",
            "729  2022-01-29 0:00:00          1.009462          1.019828   \n",
            "730  2022-01-30 0:00:00          0.993858          1.001886   \n",
            "731  2022-01-31 0:00:00          1.014843          1.032657   \n",
            "\n",
            "     DOGE-ChangeFactor  BTC-Open-EMA7  BTC-High-EMA7  BTC-Low-EMA7  \\\n",
            "0             0.981148    9508.313477    9521.706055   9230.776367   \n",
            "1             1.012965    9467.824463    9501.110352   9251.392090   \n",
            "2             1.019413    9448.323426    9493.032227   9243.000122   \n",
            "3             0.989457    9422.413468    9504.867188   9244.408539   \n",
            "4             1.026661    9390.020550    9461.466797   9211.509285   \n",
            "..                 ...            ...            ...           ...   \n",
            "727           0.982547   37538.862062   38409.071774  36182.259659   \n",
            "728           1.002917   37436.257874   38295.023558  36189.472089   \n",
            "729           1.009933   37522.372115   38365.333099  36493.722232   \n",
            "730           0.974933   37679.758579   38340.584784  36729.719409   \n",
            "731           1.016749   37739.889247   38417.254018  36730.683112   \n",
            "\n",
            "     BTC-Close-EMA7  BTC-Adj Close-EMA7  BTC-Volume-EMA7  ...  \\\n",
            "0       9350.529297         9350.529297     2.943249e+10  ...   \n",
            "1       9361.115723         9361.115723     2.855503e+10  ...   \n",
            "2       9356.928101         9356.928101     2.912521e+10  ...   \n",
            "3       9341.076446         9341.076446     2.957743e+10  ...   \n",
            "4       9301.048058         9301.048058     2.965637e+10  ...   \n",
            "..              ...                 ...              ...  ...   \n",
            "727    37438.848769        37438.848769     2.970680e+10  ...   \n",
            "728    37525.219584        37525.219584     2.783981e+10  ...   \n",
            "729    37678.459611        37678.459611     2.517840e+10  ...   \n",
            "730    37738.245098        37738.245098     2.254469e+10  ...   \n",
            "731    37924.465073        37924.465073     2.209220e+10  ...   \n",
            "\n",
            "     reddit-politicsNegScoreaverage-EMA7  reddit-politicsComScoresum-EMA7  \\\n",
            "0                               0.108745                        80.578000   \n",
            "1                               0.111572                        68.481725   \n",
            "2                               0.112800                        65.678469   \n",
            "3                               0.112592                        64.871752   \n",
            "4                               0.111291                        70.378914   \n",
            "..                                   ...                              ...   \n",
            "727                             0.124408                       -19.565339   \n",
            "728                             0.119889                        -5.280304   \n",
            "729                             0.120921                       -11.116853   \n",
            "730                             0.124151                       -15.809690   \n",
            "731                             0.130193                       -30.823117   \n",
            "\n",
            "     reddit-politicsComScoreaverage-EMA7  reddit-politicsScorecount-EMA7  \\\n",
            "0                               0.097199                      829.000000   \n",
            "1                               0.084283                      798.500000   \n",
            "2                               0.084236                      769.125000   \n",
            "3                               0.084361                      761.093750   \n",
            "4                               0.089830                      775.320312   \n",
            "..                                   ...                             ...   \n",
            "727                            -0.037122                      527.671820   \n",
            "728                            -0.010824                      533.753865   \n",
            "729                            -0.021907                      530.065399   \n",
            "730                            -0.033451                      507.299049   \n",
            "731                            -0.062349                      507.724287   \n",
            "\n",
            "     BTC-ChangeFactor-EMA7-prediction  ETH-ChangeFactor-EMA7-prediction  \\\n",
            "0                            0.000000                          0.000000   \n",
            "1                            0.000000                          0.000000   \n",
            "2                            0.000000                          0.000000   \n",
            "3                            0.000000                          0.000000   \n",
            "4                            0.000000                          0.000000   \n",
            "..                                ...                               ...   \n",
            "727                          0.972288                          0.943944   \n",
            "728                          0.980454                          0.943783   \n",
            "729                          0.982625                          0.943779   \n",
            "730                          0.982957                          0.939972   \n",
            "731                               NaN                          0.000000   \n",
            "\n",
            "     DOGE-ChangeFactor-EMA7-prediction  BTC-ChangeFactor-prediction  \\\n",
            "0                             0.000000                     0.000000   \n",
            "1                             0.000000                     0.000000   \n",
            "2                             0.000000                     0.000000   \n",
            "3                             0.000000                     0.000000   \n",
            "4                             0.000000                     0.000000   \n",
            "..                                 ...                          ...   \n",
            "727                           1.045349                     1.011986   \n",
            "728                           1.045351                     1.011986   \n",
            "729                           1.045351                     1.011986   \n",
            "730                           1.047855                     1.011986   \n",
            "731                           0.000000                     0.000000   \n",
            "\n",
            "     ETH-ChangeFactor-prediction  DOGE-ChangeFactor-prediction  \n",
            "0                       0.000000                      0.000000  \n",
            "1                       0.000000                      0.000000  \n",
            "2                       0.000000                      0.000000  \n",
            "3                       0.000000                      0.000000  \n",
            "4                       0.000000                      0.000000  \n",
            "..                           ...                           ...  \n",
            "727                     1.004704                      1.014224  \n",
            "728                     1.005917                      1.012282  \n",
            "729                     1.006684                      1.013887  \n",
            "730                     1.006806                      1.024218  \n",
            "731                     0.000000                      0.000000  \n",
            "\n",
            "[732 rows x 192 columns]\n",
            "Total number of days present in the dataset:  732\n",
            "Total number of fields present in the dataset:  192\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 732 entries, 0 to 731\n",
            "Columns: 192 entries, Date to DOGE-ChangeFactor-prediction\n",
            "dtypes: float64(191), object(1)\n",
            "memory usage: 1.1+ MB\n",
            "(732, 192)                  Date  BTC-ChangeFactor  ETH-ChangeFactor  DOGE-ChangeFactor  \\\n",
            "0  2020-01-31 0:00:00          0.983406          0.975226           0.981148   \n",
            "1  2020-02-01 0:00:00          1.004977          1.019766           1.012965   \n",
            "2  2020-02-02 0:00:00          0.995159          1.027706           1.019413   \n",
            "3  2020-02-03 0:00:00          0.994525          1.006668           0.989457   \n",
            "4  2020-02-04 0:00:00          0.987961          0.996781           1.026661   \n",
            "\n",
            "   BTC-Open-EMA7  BTC-High-EMA7  BTC-Low-EMA7  BTC-Close-EMA7  \\\n",
            "0    9508.313477    9521.706055   9230.776367     9350.529297   \n",
            "1    9467.824463    9501.110352   9251.392090     9361.115723   \n",
            "2    9448.323426    9493.032227   9243.000122     9356.928101   \n",
            "3    9422.413468    9504.867188   9244.408539     9341.076446   \n",
            "4    9390.020550    9461.466797   9211.509285     9301.048058   \n",
            "\n",
            "   BTC-Adj Close-EMA7  BTC-Volume-EMA7  ...  \\\n",
            "0         9350.529297     2.943249e+10  ...   \n",
            "1         9361.115723     2.855503e+10  ...   \n",
            "2         9356.928101     2.912521e+10  ...   \n",
            "3         9341.076446     2.957743e+10  ...   \n",
            "4         9301.048058     2.965637e+10  ...   \n",
            "\n",
            "   reddit-politicsNegScoreaverage-EMA7  reddit-politicsComScoresum-EMA7  \\\n",
            "0                             0.108745                        80.578000   \n",
            "1                             0.111572                        68.481725   \n",
            "2                             0.112800                        65.678469   \n",
            "3                             0.112592                        64.871752   \n",
            "4                             0.111291                        70.378914   \n",
            "\n",
            "   reddit-politicsComScoreaverage-EMA7  reddit-politicsScorecount-EMA7  \\\n",
            "0                             0.097199                      829.000000   \n",
            "1                             0.084283                      798.500000   \n",
            "2                             0.084236                      769.125000   \n",
            "3                             0.084361                      761.093750   \n",
            "4                             0.089830                      775.320312   \n",
            "\n",
            "   BTC-ChangeFactor-EMA7-prediction  ETH-ChangeFactor-EMA7-prediction  \\\n",
            "0                               0.0                               0.0   \n",
            "1                               0.0                               0.0   \n",
            "2                               0.0                               0.0   \n",
            "3                               0.0                               0.0   \n",
            "4                               0.0                               0.0   \n",
            "\n",
            "   DOGE-ChangeFactor-EMA7-prediction  BTC-ChangeFactor-prediction  \\\n",
            "0                                0.0                          0.0   \n",
            "1                                0.0                          0.0   \n",
            "2                                0.0                          0.0   \n",
            "3                                0.0                          0.0   \n",
            "4                                0.0                          0.0   \n",
            "\n",
            "   ETH-ChangeFactor-prediction  DOGE-ChangeFactor-prediction  \n",
            "0                          0.0                           0.0  \n",
            "1                          0.0                           0.0  \n",
            "2                          0.0                           0.0  \n",
            "3                          0.0                           0.0  \n",
            "4                          0.0                           0.0  \n",
            "\n",
            "[5 rows x 192 columns]                    Date  BTC-ChangeFactor  ETH-ChangeFactor  \\\n",
            "727  2022-01-27 0:00:00          1.008044          0.982090   \n",
            "728  2022-01-28 0:00:00          1.017665          1.051802   \n",
            "729  2022-01-29 0:00:00          1.009462          1.019828   \n",
            "730  2022-01-30 0:00:00          0.993858          1.001886   \n",
            "731  2022-01-31 0:00:00          1.014843          1.032657   \n",
            "\n",
            "     DOGE-ChangeFactor  BTC-Open-EMA7  BTC-High-EMA7  BTC-Low-EMA7  \\\n",
            "727           0.982547   37538.862062   38409.071774  36182.259659   \n",
            "728           1.002917   37436.257874   38295.023558  36189.472089   \n",
            "729           1.009933   37522.372115   38365.333099  36493.722232   \n",
            "730           0.974933   37679.758579   38340.584784  36729.719409   \n",
            "731           1.016749   37739.889247   38417.254018  36730.683112   \n",
            "\n",
            "     BTC-Close-EMA7  BTC-Adj Close-EMA7  BTC-Volume-EMA7  ...  \\\n",
            "727    37438.848769        37438.848769     2.970680e+10  ...   \n",
            "728    37525.219584        37525.219584     2.783981e+10  ...   \n",
            "729    37678.459611        37678.459611     2.517840e+10  ...   \n",
            "730    37738.245098        37738.245098     2.254469e+10  ...   \n",
            "731    37924.465073        37924.465073     2.209220e+10  ...   \n",
            "\n",
            "     reddit-politicsNegScoreaverage-EMA7  reddit-politicsComScoresum-EMA7  \\\n",
            "727                             0.124408                       -19.565339   \n",
            "728                             0.119889                        -5.280304   \n",
            "729                             0.120921                       -11.116853   \n",
            "730                             0.124151                       -15.809690   \n",
            "731                             0.130193                       -30.823117   \n",
            "\n",
            "     reddit-politicsComScoreaverage-EMA7  reddit-politicsScorecount-EMA7  \\\n",
            "727                            -0.037122                      527.671820   \n",
            "728                            -0.010824                      533.753865   \n",
            "729                            -0.021907                      530.065399   \n",
            "730                            -0.033451                      507.299049   \n",
            "731                            -0.062349                      507.724287   \n",
            "\n",
            "     BTC-ChangeFactor-EMA7-prediction  ETH-ChangeFactor-EMA7-prediction  \\\n",
            "727                          0.972288                          0.943944   \n",
            "728                          0.980454                          0.943783   \n",
            "729                          0.982625                          0.943779   \n",
            "730                          0.982957                          0.939972   \n",
            "731                               NaN                          0.000000   \n",
            "\n",
            "     DOGE-ChangeFactor-EMA7-prediction  BTC-ChangeFactor-prediction  \\\n",
            "727                           1.045349                     1.011986   \n",
            "728                           1.045351                     1.011986   \n",
            "729                           1.045351                     1.011986   \n",
            "730                           1.047855                     1.011986   \n",
            "731                           0.000000                     0.000000   \n",
            "\n",
            "     ETH-ChangeFactor-prediction  DOGE-ChangeFactor-prediction  \n",
            "727                     1.004704                      1.014224  \n",
            "728                     1.005917                      1.012282  \n",
            "729                     1.006684                      1.013887  \n",
            "730                     1.006806                      1.024218  \n",
            "731                     0.000000                      0.000000  \n",
            "\n",
            "[5 rows x 192 columns] None        BTC-ChangeFactor  ETH-ChangeFactor  DOGE-ChangeFactor  BTC-Open-EMA7  \\\n",
            "count        732.000000        732.000000         732.000000     732.000000   \n",
            "mean           1.002595          1.004976           1.011812   30461.602130   \n",
            "std            0.039927          0.053058           0.159947   19298.614209   \n",
            "min            0.628131          0.576911           0.602013    5630.800467   \n",
            "25%            0.985045          0.979477           0.975170   10417.215838   \n",
            "50%            1.002175          1.005111           1.000294   33590.641511   \n",
            "75%            1.020434          1.032652           1.019834   48079.111695   \n",
            "max            1.187972          1.259513           4.556075   64732.440897   \n",
            "\n",
            "       BTC-High-EMA7  BTC-Low-EMA7  BTC-Close-EMA7  BTC-Adj Close-EMA7  \\\n",
            "count     732.000000    732.000000      732.000000          732.000000   \n",
            "mean    31282.642561  29539.617602    30493.419400        30493.419400   \n",
            "std     19818.336396  18677.398698    19278.783862        19278.783862   \n",
            "min      5988.586282   5312.103421     5628.406731         5628.406731   \n",
            "25%     10582.725400  10205.094045    10418.403745        10418.403745   \n",
            "50%     34850.385181  32282.329801    33712.547414        33712.547414   \n",
            "75%     49212.903934  46885.806900    48125.596625        48125.596625   \n",
            "max     66053.493214  63268.913095    64733.556249        64733.556249   \n",
            "\n",
            "       BTC-Volume-EMA7  BTC-ChangeFactor-EMA7  ...  \\\n",
            "count     7.320000e+02             732.000000  ...   \n",
            "mean      4.019252e+10               1.002506  ...   \n",
            "std       1.594814e+10               0.014912  ...   \n",
            "min       1.417846e+10               0.897097  ...   \n",
            "25%       3.060710e+10               0.994403  ...   \n",
            "50%       3.658683e+10               1.002786  ...   \n",
            "75%       4.558993e+10               1.010813  ...   \n",
            "max       1.407464e+11               1.057723  ...   \n",
            "\n",
            "       reddit-politicsNegScoreaverage-EMA7  reddit-politicsComScoresum-EMA7  \\\n",
            "count                           732.000000                       732.000000   \n",
            "mean                              0.121619                        -7.765434   \n",
            "std                               0.006025                        33.522447   \n",
            "min                               0.105313                      -105.649245   \n",
            "25%                               0.117829                       -28.366292   \n",
            "50%                               0.121418                       -11.005474   \n",
            "75%                               0.125174                         6.741266   \n",
            "max                               0.145591                       114.508326   \n",
            "\n",
            "       reddit-politicsComScoreaverage-EMA7  reddit-politicsScorecount-EMA7  \\\n",
            "count                           732.000000                      732.000000   \n",
            "mean                             -0.018201                      600.078999   \n",
            "std                               0.050592                      113.758005   \n",
            "min                              -0.156466                       38.787349   \n",
            "25%                              -0.054559                      527.573614   \n",
            "50%                              -0.022107                      615.908654   \n",
            "75%                               0.009018                      674.090313   \n",
            "max                               0.145453                      829.000000   \n",
            "\n",
            "       BTC-ChangeFactor-EMA7-prediction  ETH-ChangeFactor-EMA7-prediction  \\\n",
            "count                        731.000000                        732.000000   \n",
            "mean                           0.976189                          0.949841   \n",
            "std                            0.143525                          0.147185   \n",
            "min                            0.000000                          0.000000   \n",
            "25%                            0.990274                          0.940987   \n",
            "50%                            0.994700                          0.956949   \n",
            "75%                            1.000086                          1.008711   \n",
            "max                            1.123625                          1.026691   \n",
            "\n",
            "       DOGE-ChangeFactor-EMA7-prediction  BTC-ChangeFactor-prediction  \\\n",
            "count                         732.000000                   732.000000   \n",
            "mean                            1.018250                     0.982035   \n",
            "std                             0.153712                     0.153849   \n",
            "min                             0.000000                     0.000000   \n",
            "25%                             1.030381                     1.010873   \n",
            "50%                             1.042920                     1.015207   \n",
            "75%                             1.053955                     1.016682   \n",
            "max                             1.117330                     1.076745   \n",
            "\n",
            "       ETH-ChangeFactor-prediction  DOGE-ChangeFactor-prediction  \n",
            "count                   732.000000                    732.000000  \n",
            "mean                      0.990970                      0.986437  \n",
            "std                       0.149306                      0.152403  \n",
            "min                       0.000000                      0.000000  \n",
            "25%                       1.000491                      0.991664  \n",
            "50%                       1.010035                      1.018397  \n",
            "75%                       1.024957                      1.034497  \n",
            "max                       1.064733                      1.081838  \n",
            "\n",
            "[8 rows x 191 columns] Index(['Date', 'BTC-ChangeFactor', 'ETH-ChangeFactor', 'DOGE-ChangeFactor',\n",
            "       'BTC-Open-EMA7', 'BTC-High-EMA7', 'BTC-Low-EMA7', 'BTC-Close-EMA7',\n",
            "       'BTC-Adj Close-EMA7', 'BTC-Volume-EMA7',\n",
            "       ...\n",
            "       'reddit-politicsNegScoreaverage-EMA7',\n",
            "       'reddit-politicsComScoresum-EMA7',\n",
            "       'reddit-politicsComScoreaverage-EMA7', 'reddit-politicsScorecount-EMA7',\n",
            "       'BTC-ChangeFactor-EMA7-prediction', 'ETH-ChangeFactor-EMA7-prediction',\n",
            "       'DOGE-ChangeFactor-EMA7-prediction', 'BTC-ChangeFactor-prediction',\n",
            "       'ETH-ChangeFactor-prediction', 'DOGE-ChangeFactor-prediction'],\n",
            "      dtype='object', length=192)\n",
            "\n",
            "\n",
            "\n",
            "Null Values: 1\n",
            "\n",
            "\n",
            "\n",
            "NA values: True \n",
            "\n",
            "\n",
            "\n",
            "(732, 192)\n",
            "(716, 15, 191) (716,)\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 8s 190ms/step - loss: 0.3173 - accuracy: 0.0000e+00 - val_loss: 0.0318 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 2s 124ms/step - loss: 0.1871 - accuracy: 0.0000e+00 - val_loss: 0.0316 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 2s 121ms/step - loss: 0.1474 - accuracy: 0.0000e+00 - val_loss: 0.0298 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 2s 103ms/step - loss: 0.1533 - accuracy: 0.0000e+00 - val_loss: 0.0317 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 2s 120ms/step - loss: 0.1299 - accuracy: 0.0000e+00 - val_loss: 0.0347 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 2s 103ms/step - loss: 0.0962 - accuracy: 0.0000e+00 - val_loss: 0.0426 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 2s 114ms/step - loss: 0.0937 - accuracy: 0.0000e+00 - val_loss: 0.0392 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 2s 97ms/step - loss: 0.0832 - accuracy: 0.0000e+00 - val_loss: 0.0385 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 2s 89ms/step - loss: 0.0902 - accuracy: 0.0000e+00 - val_loss: 0.0342 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 2s 89ms/step - loss: 0.0794 - accuracy: 0.0000e+00 - val_loss: 0.0379 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 1s 57ms/step - loss: 0.0741 - accuracy: 0.0000e+00 - val_loss: 0.0397 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0583 - accuracy: 0.0000e+00 - val_loss: 0.0371 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0621 - accuracy: 0.0000e+00 - val_loss: 0.0377 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 1s 52ms/step - loss: 0.0484 - accuracy: 0.0000e+00 - val_loss: 0.0344 - val_accuracy: 0.0000e+00\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 1s 52ms/step - loss: 0.0490 - accuracy: 0.0000e+00 - val_loss: 0.0342 - val_accuracy: 0.0000e+00\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.0450 - accuracy: 0.0000e+00 - val_loss: 0.0291 - val_accuracy: 0.0000e+00\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.0466 - accuracy: 0.0000e+00 - val_loss: 0.0299 - val_accuracy: 0.0000e+00\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0419 - accuracy: 0.0000e+00 - val_loss: 0.0254 - val_accuracy: 0.0000e+00\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0347 - accuracy: 0.0000e+00 - val_loss: 0.0320 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0364 - accuracy: 0.0000e+00 - val_loss: 0.0327 - val_accuracy: 0.0000e+00\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0372 - accuracy: 0.0000e+00 - val_loss: 0.0254 - val_accuracy: 0.0000e+00\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0305 - accuracy: 0.0000e+00 - val_loss: 0.0322 - val_accuracy: 0.0000e+00\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0341 - accuracy: 0.0000e+00 - val_loss: 0.0237 - val_accuracy: 0.0000e+00\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0322 - accuracy: 0.0000e+00 - val_loss: 0.0230 - val_accuracy: 0.0000e+00\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0252 - accuracy: 0.0000e+00 - val_loss: 0.0281 - val_accuracy: 0.0000e+00\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0274 - accuracy: 0.0000e+00 - val_loss: 0.0235 - val_accuracy: 0.0000e+00\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0245 - accuracy: 0.0000e+00 - val_loss: 0.0234 - val_accuracy: 0.0000e+00\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0217 - accuracy: 0.0000e+00 - val_loss: 0.0228 - val_accuracy: 0.0000e+00\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 1s 61ms/step - loss: 0.0232 - accuracy: 0.0000e+00 - val_loss: 0.0200 - val_accuracy: 0.0000e+00\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0201 - accuracy: 0.0000e+00 - val_loss: 0.0203 - val_accuracy: 0.0000e+00\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0193 - accuracy: 0.0000e+00 - val_loss: 0.0175 - val_accuracy: 0.0000e+00\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0186 - accuracy: 0.0000e+00 - val_loss: 0.0135 - val_accuracy: 0.0000e+00\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0187 - accuracy: 0.0000e+00 - val_loss: 0.0138 - val_accuracy: 0.0000e+00\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0182 - accuracy: 0.0000e+00 - val_loss: 0.0138 - val_accuracy: 0.0000e+00\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0163 - accuracy: 0.0000e+00 - val_loss: 0.0119 - val_accuracy: 0.0000e+00\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0157 - accuracy: 0.0000e+00 - val_loss: 0.0113 - val_accuracy: 0.0000e+00\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0167 - accuracy: 0.0000e+00 - val_loss: 0.0127 - val_accuracy: 0.0000e+00\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.0134 - accuracy: 0.0000e+00 - val_loss: 0.0103 - val_accuracy: 0.0000e+00\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0125 - accuracy: 0.0000e+00 - val_loss: 0.0124 - val_accuracy: 0.0000e+00\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0123 - accuracy: 0.0000e+00 - val_loss: 0.0110 - val_accuracy: 0.0000e+00\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0172 - accuracy: 0.0000e+00 - val_loss: 0.0081 - val_accuracy: 0.0000e+00\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0123 - accuracy: 0.0000e+00 - val_loss: 0.0109 - val_accuracy: 0.0000e+00\n",
            "Epoch 43/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0118 - accuracy: 0.0000e+00 - val_loss: 0.0125 - val_accuracy: 0.0000e+00\n",
            "Epoch 44/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0121 - accuracy: 0.0000e+00 - val_loss: 0.0071 - val_accuracy: 0.0000e+00\n",
            "Epoch 45/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0124 - accuracy: 0.0000e+00 - val_loss: 0.0088 - val_accuracy: 0.0000e+00\n",
            "Epoch 46/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0117 - accuracy: 0.0000e+00 - val_loss: 0.0102 - val_accuracy: 0.0000e+00\n",
            "Epoch 47/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0102 - accuracy: 0.0000e+00 - val_loss: 0.0068 - val_accuracy: 0.0000e+00\n",
            "Epoch 48/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0096 - accuracy: 0.0000e+00 - val_loss: 0.0085 - val_accuracy: 0.0000e+00\n",
            "Epoch 49/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0132 - accuracy: 0.0000e+00 - val_loss: 0.0059 - val_accuracy: 0.0000e+00\n",
            "Epoch 50/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0111 - accuracy: 0.0000e+00 - val_loss: 0.0062 - val_accuracy: 0.0000e+00\n",
            "Epoch 51/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.0087 - accuracy: 0.0000e+00 - val_loss: 0.0075 - val_accuracy: 0.0000e+00\n",
            "Epoch 52/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0092 - accuracy: 0.0000e+00 - val_loss: 0.0054 - val_accuracy: 0.0000e+00\n",
            "Epoch 53/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0097 - accuracy: 0.0000e+00 - val_loss: 0.0053 - val_accuracy: 0.0000e+00\n",
            "Epoch 54/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0071 - accuracy: 0.0000e+00 - val_loss: 0.0068 - val_accuracy: 0.0000e+00\n",
            "Epoch 55/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0069 - accuracy: 0.0000e+00 - val_loss: 0.0059 - val_accuracy: 0.0000e+00\n",
            "Epoch 56/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0070 - accuracy: 0.0000e+00 - val_loss: 0.0057 - val_accuracy: 0.0000e+00\n",
            "Epoch 57/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0076 - accuracy: 0.0000e+00 - val_loss: 0.0046 - val_accuracy: 0.0000e+00\n",
            "Epoch 58/200\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 0.0061 - accuracy: 0.0000e+00 - val_loss: 0.0045 - val_accuracy: 0.0000e+00\n",
            "Epoch 59/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0066 - accuracy: 0.0000e+00 - val_loss: 0.0027 - val_accuracy: 0.0000e+00\n",
            "Epoch 60/200\n",
            "18/18 [==============================] - 1s 52ms/step - loss: 0.0066 - accuracy: 0.0000e+00 - val_loss: 0.0047 - val_accuracy: 0.0000e+00\n",
            "Epoch 61/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0067 - accuracy: 0.0000e+00 - val_loss: 0.0046 - val_accuracy: 0.0000e+00\n",
            "Epoch 62/200\n",
            "18/18 [==============================] - 1s 57ms/step - loss: 0.0082 - accuracy: 0.0000e+00 - val_loss: 0.0052 - val_accuracy: 0.0000e+00\n",
            "Epoch 63/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0059 - accuracy: 0.0000e+00 - val_loss: 0.0042 - val_accuracy: 0.0000e+00\n",
            "Epoch 64/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0062 - accuracy: 0.0000e+00 - val_loss: 0.0044 - val_accuracy: 0.0000e+00\n",
            "Epoch 65/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0055 - accuracy: 0.0000e+00 - val_loss: 0.0038 - val_accuracy: 0.0000e+00\n",
            "Epoch 66/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0059 - accuracy: 0.0000e+00 - val_loss: 0.0044 - val_accuracy: 0.0000e+00\n",
            "Epoch 67/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0068 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
            "Epoch 68/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0054 - accuracy: 0.0000e+00 - val_loss: 0.0029 - val_accuracy: 0.0000e+00\n",
            "Epoch 69/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0067 - accuracy: 0.0000e+00 - val_loss: 0.0038 - val_accuracy: 0.0000e+00\n",
            "Epoch 70/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.0047 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
            "Epoch 71/200\n",
            "18/18 [==============================] - 1s 57ms/step - loss: 0.0049 - accuracy: 0.0000e+00 - val_loss: 0.0038 - val_accuracy: 0.0000e+00\n",
            "Epoch 72/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0042 - accuracy: 0.0000e+00 - val_loss: 0.0025 - val_accuracy: 0.0000e+00\n",
            "Epoch 73/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0056 - accuracy: 0.0000e+00 - val_loss: 0.0025 - val_accuracy: 0.0000e+00\n",
            "Epoch 74/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0050 - accuracy: 0.0000e+00 - val_loss: 0.0027 - val_accuracy: 0.0000e+00\n",
            "Epoch 75/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0052 - accuracy: 0.0000e+00 - val_loss: 0.0026 - val_accuracy: 0.0000e+00\n",
            "Epoch 76/200\n",
            "18/18 [==============================] - 1s 57ms/step - loss: 0.0047 - accuracy: 0.0000e+00 - val_loss: 0.0021 - val_accuracy: 0.0000e+00\n",
            "Epoch 77/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.0047 - accuracy: 0.0000e+00 - val_loss: 0.0021 - val_accuracy: 0.0000e+00\n",
            "Epoch 78/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0057 - accuracy: 0.0000e+00 - val_loss: 0.0026 - val_accuracy: 0.0000e+00\n",
            "Epoch 79/200\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 0.0041 - accuracy: 0.0000e+00 - val_loss: 0.0020 - val_accuracy: 0.0000e+00\n",
            "Epoch 80/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.0043 - accuracy: 0.0000e+00 - val_loss: 0.0028 - val_accuracy: 0.0000e+00\n",
            "Epoch 81/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0045 - accuracy: 0.0000e+00 - val_loss: 0.0018 - val_accuracy: 0.0000e+00\n",
            "Epoch 82/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0050 - accuracy: 0.0000e+00 - val_loss: 0.0021 - val_accuracy: 0.0000e+00\n",
            "Epoch 83/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.0054 - accuracy: 0.0000e+00 - val_loss: 0.0018 - val_accuracy: 0.0000e+00\n",
            "Epoch 84/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0045 - accuracy: 0.0000e+00 - val_loss: 0.0019 - val_accuracy: 0.0000e+00\n",
            "Epoch 85/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0019 - val_accuracy: 0.0000e+00\n",
            "Epoch 86/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.0037 - accuracy: 0.0000e+00 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
            "Epoch 87/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0034 - accuracy: 0.0000e+00 - val_loss: 0.0018 - val_accuracy: 0.0000e+00\n",
            "Epoch 88/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0034 - accuracy: 0.0000e+00 - val_loss: 0.0019 - val_accuracy: 0.0000e+00\n",
            "Epoch 89/200\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
            "Epoch 90/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
            "Epoch 91/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0041 - accuracy: 0.0000e+00 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
            "Epoch 92/200\n",
            "18/18 [==============================] - 1s 58ms/step - loss: 0.0034 - accuracy: 0.0000e+00 - val_loss: 0.0015 - val_accuracy: 0.0000e+00\n",
            "Epoch 93/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.0033 - accuracy: 0.0000e+00 - val_loss: 0.0017 - val_accuracy: 0.0000e+00\n",
            "Epoch 94/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.0030 - accuracy: 0.0000e+00 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
            "Epoch 95/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.0026 - accuracy: 0.0000e+00 - val_loss: 7.8222e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 96/200\n",
            "18/18 [==============================] - 1s 61ms/step - loss: 0.0041 - accuracy: 0.0000e+00 - val_loss: 8.3119e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 97/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0031 - accuracy: 0.0000e+00 - val_loss: 0.0019 - val_accuracy: 0.0000e+00\n",
            "Epoch 98/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0027 - accuracy: 0.0000e+00 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
            "Epoch 99/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.0028 - accuracy: 0.0000e+00 - val_loss: 8.7015e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 100/200\n",
            "18/18 [==============================] - 1s 85ms/step - loss: 0.0029 - accuracy: 0.0000e+00 - val_loss: 9.3647e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 101/200\n",
            "18/18 [==============================] - 1s 57ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 8.6007e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 102/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0028 - accuracy: 0.0000e+00 - val_loss: 8.0218e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 103/200\n",
            "18/18 [==============================] - 1s 57ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 5.9649e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 104/200\n",
            "18/18 [==============================] - 1s 57ms/step - loss: 0.0033 - accuracy: 0.0000e+00 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
            "Epoch 105/200\n",
            "18/18 [==============================] - 1s 57ms/step - loss: 0.0029 - accuracy: 0.0000e+00 - val_loss: 7.1560e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 106/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.0026 - accuracy: 0.0000e+00 - val_loss: 3.9342e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 107/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0025 - accuracy: 0.0000e+00 - val_loss: 3.9774e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 108/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0031 - accuracy: 0.0000e+00 - val_loss: 3.2085e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 109/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.0024 - accuracy: 0.0000e+00 - val_loss: 4.2543e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 110/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0028 - accuracy: 0.0000e+00 - val_loss: 5.4154e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 111/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0024 - accuracy: 0.0000e+00 - val_loss: 6.5940e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 112/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0028 - accuracy: 0.0000e+00 - val_loss: 6.8800e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 113/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0032 - accuracy: 0.0000e+00 - val_loss: 5.3731e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 114/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0026 - accuracy: 0.0000e+00 - val_loss: 3.5007e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 115/200\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 0.0022 - accuracy: 0.0000e+00 - val_loss: 6.1425e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 116/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.0025 - accuracy: 0.0000e+00 - val_loss: 5.5918e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 117/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.0024 - accuracy: 0.0000e+00 - val_loss: 4.8360e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 118/200\n",
            "18/18 [==============================] - 1s 57ms/step - loss: 0.0029 - accuracy: 0.0000e+00 - val_loss: 3.6532e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 119/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.0025 - accuracy: 0.0000e+00 - val_loss: 5.9654e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 120/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0028 - accuracy: 0.0000e+00 - val_loss: 3.2205e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 121/200\n",
            "18/18 [==============================] - 1s 58ms/step - loss: 0.0023 - accuracy: 0.0000e+00 - val_loss: 5.8569e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 122/200\n",
            "18/18 [==============================] - 1s 58ms/step - loss: 0.0025 - accuracy: 0.0000e+00 - val_loss: 3.5558e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 123/200\n",
            "18/18 [==============================] - 1s 58ms/step - loss: 0.0023 - accuracy: 0.0000e+00 - val_loss: 5.5674e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 124/200\n",
            "18/18 [==============================] - 1s 60ms/step - loss: 0.0021 - accuracy: 0.0000e+00 - val_loss: 4.8652e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 125/200\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 0.0026 - accuracy: 0.0000e+00 - val_loss: 3.1425e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 126/200\n",
            "18/18 [==============================] - 1s 57ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 2.3837e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 127/200\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 0.0022 - accuracy: 0.0000e+00 - val_loss: 2.9871e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 128/200\n",
            "18/18 [==============================] - 1s 57ms/step - loss: 0.0022 - accuracy: 0.0000e+00 - val_loss: 2.4317e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 129/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0023 - accuracy: 0.0000e+00 - val_loss: 2.3630e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 130/200\n",
            "18/18 [==============================] - 1s 58ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 2.2293e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 131/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0026 - accuracy: 0.0000e+00 - val_loss: 2.7520e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 132/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0024 - accuracy: 0.0000e+00 - val_loss: 3.8765e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 133/200\n",
            "18/18 [==============================] - 1s 57ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 6.2824e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 134/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0021 - accuracy: 0.0000e+00 - val_loss: 6.1784e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 135/200\n",
            "18/18 [==============================] - 1s 57ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 2.1994e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 136/200\n",
            "18/18 [==============================] - 1s 58ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 2.1177e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 137/200\n",
            "18/18 [==============================] - 1s 60ms/step - loss: 0.0033 - accuracy: 0.0000e+00 - val_loss: 3.6923e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 138/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 2.2678e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 139/200\n",
            "18/18 [==============================] - 1s 57ms/step - loss: 0.0023 - accuracy: 0.0000e+00 - val_loss: 2.8390e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 140/200\n",
            "18/18 [==============================] - 1s 57ms/step - loss: 0.0024 - accuracy: 0.0000e+00 - val_loss: 3.2812e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 141/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0022 - accuracy: 0.0000e+00 - val_loss: 2.4485e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 142/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.0024 - accuracy: 0.0000e+00 - val_loss: 2.0379e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 143/200\n",
            "18/18 [==============================] - 1s 57ms/step - loss: 0.0018 - accuracy: 0.0000e+00 - val_loss: 2.3658e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 144/200\n",
            "18/18 [==============================] - 1s 58ms/step - loss: 0.0017 - accuracy: 0.0000e+00 - val_loss: 2.2843e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 145/200\n",
            "18/18 [==============================] - 1s 60ms/step - loss: 0.0018 - accuracy: 0.0000e+00 - val_loss: 2.9611e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 146/200\n",
            "18/18 [==============================] - 1s 58ms/step - loss: 0.0024 - accuracy: 0.0000e+00 - val_loss: 2.4655e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 147/200\n",
            "18/18 [==============================] - 1s 57ms/step - loss: 0.0017 - accuracy: 0.0000e+00 - val_loss: 3.6488e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 148/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.0016 - accuracy: 0.0000e+00 - val_loss: 2.4098e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 149/200\n",
            "18/18 [==============================] - 1s 57ms/step - loss: 0.0017 - accuracy: 0.0000e+00 - val_loss: 2.1612e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 150/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.0023 - accuracy: 0.0000e+00 - val_loss: 2.5766e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 151/200\n",
            "18/18 [==============================] - 1s 57ms/step - loss: 0.0016 - accuracy: 0.0000e+00 - val_loss: 3.3746e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 152/200\n",
            "18/18 [==============================] - 1s 58ms/step - loss: 0.0016 - accuracy: 0.0000e+00 - val_loss: 2.0821e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 153/200\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 2.0763e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 154/200\n",
            "18/18 [==============================] - 1s 60ms/step - loss: 0.0021 - accuracy: 0.0000e+00 - val_loss: 2.0187e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 155/200\n",
            "18/18 [==============================] - 1s 57ms/step - loss: 0.0015 - accuracy: 0.0000e+00 - val_loss: 1.8927e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 156/200\n",
            "18/18 [==============================] - 1s 58ms/step - loss: 0.0016 - accuracy: 0.0000e+00 - val_loss: 2.2143e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 157/200\n",
            "18/18 [==============================] - 1s 58ms/step - loss: 0.0017 - accuracy: 0.0000e+00 - val_loss: 1.9224e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 158/200\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 1.9628e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 159/200\n",
            "18/18 [==============================] - 1s 60ms/step - loss: 0.0014 - accuracy: 0.0000e+00 - val_loss: 1.9744e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 160/200\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 0.0021 - accuracy: 0.0000e+00 - val_loss: 2.1976e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 161/200\n",
            "18/18 [==============================] - 1s 58ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 1.9793e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 162/200\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 0.0013 - accuracy: 0.0000e+00 - val_loss: 2.0177e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 163/200\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 0.0018 - accuracy: 0.0000e+00 - val_loss: 1.9913e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 164/200\n",
            "18/18 [==============================] - 1s 60ms/step - loss: 0.0016 - accuracy: 0.0000e+00 - val_loss: 2.2878e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 165/200\n",
            "18/18 [==============================] - 1s 61ms/step - loss: 0.0014 - accuracy: 0.0000e+00 - val_loss: 2.8978e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 166/200\n",
            "18/18 [==============================] - 1s 61ms/step - loss: 0.0013 - accuracy: 0.0000e+00 - val_loss: 2.0697e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 167/200\n",
            "18/18 [==============================] - 1s 60ms/step - loss: 0.0016 - accuracy: 0.0000e+00 - val_loss: 2.4301e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 168/200\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 0.0015 - accuracy: 0.0000e+00 - val_loss: 1.7880e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 169/200\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 0.0014 - accuracy: 0.0000e+00 - val_loss: 1.7239e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 170/200\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 0.0013 - accuracy: 0.0000e+00 - val_loss: 2.4830e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 171/200\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 0.0013 - accuracy: 0.0000e+00 - val_loss: 1.7760e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 172/200\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 0.0013 - accuracy: 0.0000e+00 - val_loss: 2.0243e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 173/200\n",
            "18/18 [==============================] - 1s 60ms/step - loss: 0.0016 - accuracy: 0.0000e+00 - val_loss: 1.9333e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 174/200\n",
            "18/18 [==============================] - 1s 60ms/step - loss: 0.0014 - accuracy: 0.0000e+00 - val_loss: 1.9172e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 175/200\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 0.0018 - accuracy: 0.0000e+00 - val_loss: 2.9849e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 176/200\n",
            "18/18 [==============================] - 1s 62ms/step - loss: 0.0016 - accuracy: 0.0000e+00 - val_loss: 2.0203e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 177/200\n",
            "18/18 [==============================] - 1s 61ms/step - loss: 0.0015 - accuracy: 0.0000e+00 - val_loss: 2.6685e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 178/200\n",
            "18/18 [==============================] - 1s 60ms/step - loss: 0.0018 - accuracy: 0.0000e+00 - val_loss: 2.1828e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 179/200\n",
            "18/18 [==============================] - 1s 62ms/step - loss: 0.0015 - accuracy: 0.0000e+00 - val_loss: 1.9988e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 180/200\n",
            "18/18 [==============================] - 1s 60ms/step - loss: 0.0012 - accuracy: 0.0000e+00 - val_loss: 2.2822e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 181/200\n",
            "18/18 [==============================] - 1s 60ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 1.7399e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 182/200\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 0.0014 - accuracy: 0.0000e+00 - val_loss: 1.7594e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 183/200\n",
            "18/18 [==============================] - 1s 60ms/step - loss: 0.0015 - accuracy: 0.0000e+00 - val_loss: 1.6709e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 184/200\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 0.0013 - accuracy: 0.0000e+00 - val_loss: 1.8794e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 185/200\n",
            "18/18 [==============================] - 1s 58ms/step - loss: 0.0012 - accuracy: 0.0000e+00 - val_loss: 1.9247e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 186/200\n",
            "18/18 [==============================] - 1s 60ms/step - loss: 0.0012 - accuracy: 0.0000e+00 - val_loss: 1.9789e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 187/200\n",
            "18/18 [==============================] - 1s 60ms/step - loss: 0.0012 - accuracy: 0.0000e+00 - val_loss: 2.2595e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 188/200\n",
            "18/18 [==============================] - 1s 57ms/step - loss: 0.0013 - accuracy: 0.0000e+00 - val_loss: 2.9275e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 189/200\n",
            "18/18 [==============================] - 1s 58ms/step - loss: 0.0013 - accuracy: 0.0000e+00 - val_loss: 2.3644e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 190/200\n",
            "18/18 [==============================] - 1s 60ms/step - loss: 0.0014 - accuracy: 0.0000e+00 - val_loss: 1.7009e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 191/200\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 0.0013 - accuracy: 0.0000e+00 - val_loss: 2.6023e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 192/200\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 0.0016 - accuracy: 0.0000e+00 - val_loss: 3.2546e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 193/200\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 0.0017 - accuracy: 0.0000e+00 - val_loss: 1.7251e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 194/200\n",
            "18/18 [==============================] - 1s 57ms/step - loss: 0.0014 - accuracy: 0.0000e+00 - val_loss: 1.7663e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 195/200\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 0.0011 - accuracy: 0.0000e+00 - val_loss: 2.1048e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 196/200\n",
            "18/18 [==============================] - 1s 60ms/step - loss: 0.0011 - accuracy: 0.0000e+00 - val_loss: 2.2278e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 197/200\n",
            "18/18 [==============================] - 1s 58ms/step - loss: 0.0014 - accuracy: 0.0000e+00 - val_loss: 2.5991e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 198/200\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 0.0013 - accuracy: 0.0000e+00 - val_loss: 1.8886e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 199/200\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 0.0014 - accuracy: 0.0000e+00 - val_loss: 2.6421e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 200/200\n",
            "18/18 [==============================] - 1s 58ms/step - loss: 0.0011 - accuracy: 0.0000e+00 - val_loss: 1.7499e-04 - val_accuracy: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(572,) (144,)\n",
            "Train data RMSE:  0.039189268568668736\n",
            "Train data MSE:  0.0015357987709472473\n",
            "Train data MAE:  0.02123285538623697\n",
            "-------------------------------------------------------------------------------------\n",
            "Test data RMSE:  0.013228448225789315\n",
            "Test data MSE:  0.00017499184246238847\n",
            "Test data MAE:  0.01012220124099659\n",
            "Train data R2 score: -5.577054347422748\n",
            "Test data R2 score: -0.04299421803482972\n",
            "Train data MGD:  0.0018522625497210934\n",
            "Test data MGD:  0.0001753884802104471\n",
            "----------------------------------------------------------------------\n",
            "Train data MPD:  0.001682720436506251\n",
            "Test data MPD:  0.0001751814203976098\n",
            "(716, 15, 191) (716,)\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 4s 74ms/step - loss: 1.3664 - accuracy: 0.0000e+00 - val_loss: 0.7660 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.6590 - accuracy: 0.0000e+00 - val_loss: 0.3161 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 1s 52ms/step - loss: 0.4596 - accuracy: 0.0000e+00 - val_loss: 0.1539 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 1s 52ms/step - loss: 0.3707 - accuracy: 0.0000e+00 - val_loss: 0.1101 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.2743 - accuracy: 0.0000e+00 - val_loss: 0.0986 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.2628 - accuracy: 0.0000e+00 - val_loss: 0.0949 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.2670 - accuracy: 0.0000e+00 - val_loss: 0.0909 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.2528 - accuracy: 0.0000e+00 - val_loss: 0.0865 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 1s 51ms/step - loss: 0.1937 - accuracy: 0.0000e+00 - val_loss: 0.0843 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.1986 - accuracy: 0.0000e+00 - val_loss: 0.0835 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.1784 - accuracy: 0.0000e+00 - val_loss: 0.0835 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 1s 52ms/step - loss: 0.1638 - accuracy: 0.0000e+00 - val_loss: 0.0814 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.1496 - accuracy: 0.0000e+00 - val_loss: 0.0799 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 1s 52ms/step - loss: 0.1378 - accuracy: 0.0000e+00 - val_loss: 0.0753 - val_accuracy: 0.0000e+00\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.1438 - accuracy: 0.0000e+00 - val_loss: 0.0710 - val_accuracy: 0.0000e+00\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.1126 - accuracy: 0.0000e+00 - val_loss: 0.0701 - val_accuracy: 0.0000e+00\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.1112 - accuracy: 0.0000e+00 - val_loss: 0.0666 - val_accuracy: 0.0000e+00\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0911 - accuracy: 0.0000e+00 - val_loss: 0.0657 - val_accuracy: 0.0000e+00\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.1076 - accuracy: 0.0000e+00 - val_loss: 0.0681 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0869 - accuracy: 0.0000e+00 - val_loss: 0.0638 - val_accuracy: 0.0000e+00\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0855 - accuracy: 0.0000e+00 - val_loss: 0.0612 - val_accuracy: 0.0000e+00\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0847 - accuracy: 0.0000e+00 - val_loss: 0.0566 - val_accuracy: 0.0000e+00\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0850 - accuracy: 0.0000e+00 - val_loss: 0.0521 - val_accuracy: 0.0000e+00\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0685 - accuracy: 0.0000e+00 - val_loss: 0.0526 - val_accuracy: 0.0000e+00\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0682 - accuracy: 0.0000e+00 - val_loss: 0.0491 - val_accuracy: 0.0000e+00\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0614 - accuracy: 0.0000e+00 - val_loss: 0.0474 - val_accuracy: 0.0000e+00\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 1s 51ms/step - loss: 0.0662 - accuracy: 0.0000e+00 - val_loss: 0.0443 - val_accuracy: 0.0000e+00\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 1s 52ms/step - loss: 0.0661 - accuracy: 0.0000e+00 - val_loss: 0.0433 - val_accuracy: 0.0000e+00\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0571 - accuracy: 0.0000e+00 - val_loss: 0.0381 - val_accuracy: 0.0000e+00\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0516 - accuracy: 0.0000e+00 - val_loss: 0.0361 - val_accuracy: 0.0000e+00\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.0536 - accuracy: 0.0000e+00 - val_loss: 0.0359 - val_accuracy: 0.0000e+00\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0549 - accuracy: 0.0000e+00 - val_loss: 0.0334 - val_accuracy: 0.0000e+00\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0471 - accuracy: 0.0000e+00 - val_loss: 0.0324 - val_accuracy: 0.0000e+00\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0384 - accuracy: 0.0000e+00 - val_loss: 0.0315 - val_accuracy: 0.0000e+00\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0406 - accuracy: 0.0000e+00 - val_loss: 0.0293 - val_accuracy: 0.0000e+00\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0445 - accuracy: 0.0000e+00 - val_loss: 0.0263 - val_accuracy: 0.0000e+00\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0416 - accuracy: 0.0000e+00 - val_loss: 0.0250 - val_accuracy: 0.0000e+00\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0396 - accuracy: 0.0000e+00 - val_loss: 0.0268 - val_accuracy: 0.0000e+00\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0344 - accuracy: 0.0000e+00 - val_loss: 0.0244 - val_accuracy: 0.0000e+00\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0344 - accuracy: 0.0000e+00 - val_loss: 0.0220 - val_accuracy: 0.0000e+00\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0373 - accuracy: 0.0000e+00 - val_loss: 0.0204 - val_accuracy: 0.0000e+00\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0318 - accuracy: 0.0000e+00 - val_loss: 0.0188 - val_accuracy: 0.0000e+00\n",
            "Epoch 43/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0331 - accuracy: 0.0000e+00 - val_loss: 0.0175 - val_accuracy: 0.0000e+00\n",
            "Epoch 44/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0302 - accuracy: 0.0000e+00 - val_loss: 0.0167 - val_accuracy: 0.0000e+00\n",
            "Epoch 45/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0277 - accuracy: 0.0000e+00 - val_loss: 0.0162 - val_accuracy: 0.0000e+00\n",
            "Epoch 46/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0266 - accuracy: 0.0000e+00 - val_loss: 0.0150 - val_accuracy: 0.0000e+00\n",
            "Epoch 47/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0301 - accuracy: 0.0000e+00 - val_loss: 0.0136 - val_accuracy: 0.0000e+00\n",
            "Epoch 48/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0302 - accuracy: 0.0000e+00 - val_loss: 0.0119 - val_accuracy: 0.0000e+00\n",
            "Epoch 49/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0254 - accuracy: 0.0000e+00 - val_loss: 0.0120 - val_accuracy: 0.0000e+00\n",
            "Epoch 50/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0233 - accuracy: 0.0000e+00 - val_loss: 0.0110 - val_accuracy: 0.0000e+00\n",
            "Epoch 51/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0236 - accuracy: 0.0000e+00 - val_loss: 0.0101 - val_accuracy: 0.0000e+00\n",
            "Epoch 52/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - val_loss: 0.0096 - val_accuracy: 0.0000e+00\n",
            "Epoch 53/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0190 - accuracy: 0.0000e+00 - val_loss: 0.0100 - val_accuracy: 0.0000e+00\n",
            "Epoch 54/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - val_loss: 0.0089 - val_accuracy: 0.0000e+00\n",
            "Epoch 55/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - val_loss: 0.0084 - val_accuracy: 0.0000e+00\n",
            "Epoch 56/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0177 - accuracy: 0.0000e+00 - val_loss: 0.0084 - val_accuracy: 0.0000e+00\n",
            "Epoch 57/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0189 - accuracy: 0.0000e+00 - val_loss: 0.0078 - val_accuracy: 0.0000e+00\n",
            "Epoch 58/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0201 - accuracy: 0.0000e+00 - val_loss: 0.0072 - val_accuracy: 0.0000e+00\n",
            "Epoch 59/200\n",
            "18/18 [==============================] - 2s 91ms/step - loss: 0.0180 - accuracy: 0.0000e+00 - val_loss: 0.0070 - val_accuracy: 0.0000e+00\n",
            "Epoch 60/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0162 - accuracy: 0.0000e+00 - val_loss: 0.0066 - val_accuracy: 0.0000e+00\n",
            "Epoch 61/200\n",
            "18/18 [==============================] - 1s 52ms/step - loss: 0.0178 - accuracy: 0.0000e+00 - val_loss: 0.0065 - val_accuracy: 0.0000e+00\n",
            "Epoch 62/200\n",
            "18/18 [==============================] - 1s 57ms/step - loss: 0.0167 - accuracy: 0.0000e+00 - val_loss: 0.0061 - val_accuracy: 0.0000e+00\n",
            "Epoch 63/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0151 - accuracy: 0.0000e+00 - val_loss: 0.0062 - val_accuracy: 0.0000e+00\n",
            "Epoch 64/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0143 - accuracy: 0.0000e+00 - val_loss: 0.0057 - val_accuracy: 0.0000e+00\n",
            "Epoch 65/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0155 - accuracy: 0.0000e+00 - val_loss: 0.0053 - val_accuracy: 0.0000e+00\n",
            "Epoch 66/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.0126 - accuracy: 0.0000e+00 - val_loss: 0.0051 - val_accuracy: 0.0000e+00\n",
            "Epoch 67/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0148 - accuracy: 0.0000e+00 - val_loss: 0.0047 - val_accuracy: 0.0000e+00\n",
            "Epoch 68/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0138 - accuracy: 0.0000e+00 - val_loss: 0.0047 - val_accuracy: 0.0000e+00\n",
            "Epoch 69/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0131 - accuracy: 0.0000e+00 - val_loss: 0.0041 - val_accuracy: 0.0000e+00\n",
            "Epoch 70/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0126 - accuracy: 0.0000e+00 - val_loss: 0.0039 - val_accuracy: 0.0000e+00\n",
            "Epoch 71/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0129 - accuracy: 0.0000e+00 - val_loss: 0.0041 - val_accuracy: 0.0000e+00\n",
            "Epoch 72/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0120 - accuracy: 0.0000e+00 - val_loss: 0.0038 - val_accuracy: 0.0000e+00\n",
            "Epoch 73/200\n",
            "18/18 [==============================] - 2s 95ms/step - loss: 0.0113 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
            "Epoch 74/200\n",
            "18/18 [==============================] - 2s 119ms/step - loss: 0.0115 - accuracy: 0.0000e+00 - val_loss: 0.0033 - val_accuracy: 0.0000e+00\n",
            "Epoch 75/200\n",
            "18/18 [==============================] - 2s 108ms/step - loss: 0.0103 - accuracy: 0.0000e+00 - val_loss: 0.0033 - val_accuracy: 0.0000e+00\n",
            "Epoch 76/200\n",
            "18/18 [==============================] - 2s 109ms/step - loss: 0.0111 - accuracy: 0.0000e+00 - val_loss: 0.0032 - val_accuracy: 0.0000e+00\n",
            "Epoch 77/200\n",
            "18/18 [==============================] - 2s 110ms/step - loss: 0.0108 - accuracy: 0.0000e+00 - val_loss: 0.0031 - val_accuracy: 0.0000e+00\n",
            "Epoch 78/200\n",
            "18/18 [==============================] - 2s 107ms/step - loss: 0.0106 - accuracy: 0.0000e+00 - val_loss: 0.0029 - val_accuracy: 0.0000e+00\n",
            "Epoch 79/200\n",
            "18/18 [==============================] - 2s 128ms/step - loss: 0.0099 - accuracy: 0.0000e+00 - val_loss: 0.0027 - val_accuracy: 0.0000e+00\n",
            "Epoch 80/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.0111 - accuracy: 0.0000e+00 - val_loss: 0.0029 - val_accuracy: 0.0000e+00\n",
            "Epoch 81/200\n",
            "18/18 [==============================] - 2s 119ms/step - loss: 0.0091 - accuracy: 0.0000e+00 - val_loss: 0.0029 - val_accuracy: 0.0000e+00\n",
            "Epoch 82/200\n",
            "18/18 [==============================] - 1s 78ms/step - loss: 0.0091 - accuracy: 0.0000e+00 - val_loss: 0.0026 - val_accuracy: 0.0000e+00\n",
            "Epoch 83/200\n",
            "18/18 [==============================] - 1s 52ms/step - loss: 0.0099 - accuracy: 0.0000e+00 - val_loss: 0.0024 - val_accuracy: 0.0000e+00\n",
            "Epoch 84/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0089 - accuracy: 0.0000e+00 - val_loss: 0.0022 - val_accuracy: 0.0000e+00\n",
            "Epoch 85/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0084 - accuracy: 0.0000e+00 - val_loss: 0.0025 - val_accuracy: 0.0000e+00\n",
            "Epoch 86/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0089 - accuracy: 0.0000e+00 - val_loss: 0.0022 - val_accuracy: 0.0000e+00\n",
            "Epoch 87/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0087 - accuracy: 0.0000e+00 - val_loss: 0.0021 - val_accuracy: 0.0000e+00\n",
            "Epoch 88/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0081 - accuracy: 0.0000e+00 - val_loss: 0.0023 - val_accuracy: 0.0000e+00\n",
            "Epoch 89/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0087 - accuracy: 0.0000e+00 - val_loss: 0.0019 - val_accuracy: 0.0000e+00\n",
            "Epoch 90/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0086 - accuracy: 0.0000e+00 - val_loss: 0.0019 - val_accuracy: 0.0000e+00\n",
            "Epoch 91/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0084 - accuracy: 0.0000e+00 - val_loss: 0.0018 - val_accuracy: 0.0000e+00\n",
            "Epoch 92/200\n",
            "18/18 [==============================] - 1s 58ms/step - loss: 0.0072 - accuracy: 0.0000e+00 - val_loss: 0.0018 - val_accuracy: 0.0000e+00\n",
            "Epoch 93/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0081 - accuracy: 0.0000e+00 - val_loss: 0.0018 - val_accuracy: 0.0000e+00\n",
            "Epoch 94/200\n",
            "18/18 [==============================] - 1s 58ms/step - loss: 0.0075 - accuracy: 0.0000e+00 - val_loss: 0.0017 - val_accuracy: 0.0000e+00\n",
            "Epoch 95/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0075 - accuracy: 0.0000e+00 - val_loss: 0.0016 - val_accuracy: 0.0000e+00\n",
            "Epoch 96/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0088 - accuracy: 0.0000e+00 - val_loss: 0.0017 - val_accuracy: 0.0000e+00\n",
            "Epoch 97/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0089 - accuracy: 0.0000e+00 - val_loss: 0.0015 - val_accuracy: 0.0000e+00\n",
            "Epoch 98/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0070 - accuracy: 0.0000e+00 - val_loss: 0.0015 - val_accuracy: 0.0000e+00\n",
            "Epoch 99/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0069 - accuracy: 0.0000e+00 - val_loss: 0.0014 - val_accuracy: 0.0000e+00\n",
            "Epoch 100/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0069 - accuracy: 0.0000e+00 - val_loss: 0.0015 - val_accuracy: 0.0000e+00\n",
            "Epoch 101/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0064 - accuracy: 0.0000e+00 - val_loss: 0.0015 - val_accuracy: 0.0000e+00\n",
            "Epoch 102/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0067 - accuracy: 0.0000e+00 - val_loss: 0.0015 - val_accuracy: 0.0000e+00\n",
            "Epoch 103/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.0078 - accuracy: 0.0000e+00 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
            "Epoch 104/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.0074 - accuracy: 0.0000e+00 - val_loss: 0.0014 - val_accuracy: 0.0000e+00\n",
            "Epoch 105/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.0066 - accuracy: 0.0000e+00 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
            "Epoch 106/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0061 - accuracy: 0.0000e+00 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
            "Epoch 107/200\n",
            "18/18 [==============================] - 1s 57ms/step - loss: 0.0063 - accuracy: 0.0000e+00 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
            "Epoch 108/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.0064 - accuracy: 0.0000e+00 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
            "Epoch 109/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0061 - accuracy: 0.0000e+00 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
            "Epoch 110/200\n",
            "18/18 [==============================] - 1s 52ms/step - loss: 0.0059 - accuracy: 0.0000e+00 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
            "Epoch 111/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0056 - accuracy: 0.0000e+00 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
            "Epoch 112/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0059 - accuracy: 0.0000e+00 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
            "Epoch 113/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.0054 - accuracy: 0.0000e+00 - val_loss: 8.9392e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 114/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.0051 - accuracy: 0.0000e+00 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
            "Epoch 115/200\n",
            "18/18 [==============================] - 1s 58ms/step - loss: 0.0056 - accuracy: 0.0000e+00 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
            "Epoch 116/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0062 - accuracy: 0.0000e+00 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
            "Epoch 117/200\n",
            "18/18 [==============================] - 1s 58ms/step - loss: 0.0053 - accuracy: 0.0000e+00 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
            "Epoch 118/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0054 - accuracy: 0.0000e+00 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
            "Epoch 119/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0047 - accuracy: 0.0000e+00 - val_loss: 9.2691e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 120/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0053 - accuracy: 0.0000e+00 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
            "Epoch 121/200\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0052 - accuracy: 0.0000e+00 - val_loss: 7.3005e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 122/200\n",
            "18/18 [==============================] - 1s 54ms/step - loss: 0.0055 - accuracy: 0.0000e+00 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
            "Epoch 123/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.0048 - accuracy: 0.0000e+00 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
            "Epoch 124/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.0058 - accuracy: 0.0000e+00 - val_loss: 8.8419e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 125/200\n",
            "18/18 [==============================] - 1s 58ms/step - loss: 0.0046 - accuracy: 0.0000e+00 - val_loss: 8.0112e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 126/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 0.0043 - accuracy: 0.0000e+00 - val_loss: 8.9349e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 127/200\n",
            "18/18 [==============================] - 1s 62ms/step - loss: 0.0041 - accuracy: 0.0000e+00 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
            "Epoch 128/200\n",
            "18/18 [==============================] - 2s 85ms/step - loss: 0.0045 - accuracy: 0.0000e+00 - val_loss: 8.3880e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 129/200\n",
            "18/18 [==============================] - 2s 111ms/step - loss: 0.0042 - accuracy: 0.0000e+00 - val_loss: 7.7626e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 130/200\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.0042 - accuracy: 0.0000e+00 - val_loss: 9.2035e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 131/200\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 0.0042 - accuracy: 0.0000e+00 - val_loss: 7.3148e-04 - val_accuracy: 0.0000e+00\n",
            "Epoch 132/200\n",
            "13/18 [====================>.........] - ETA: 0s - loss: 0.0042 - accuracy: 0.0000e+00"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"LSTMPredictor.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1snuaWc20AeWGvnrBaj0MKyRIcmX3l6jS\n",
        "\"\"\"\n",
        "\n",
        "# !pip install ipdb\n",
        "# First we will import the necessary Library\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import datetime as dt\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "# For Evalution we will use these library\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score, r2_score\n",
        "from sklearn.metrics import mean_poisson_deviance, mean_gamma_deviance, accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# For model building we will use these library\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "\n",
        "# For PLotting we will use these library\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import cycle\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/MyDrive')\n",
        "\n",
        "def main(filepath: str, filename: str, prediction_columns: list, time_step: int = 10, num_units: int = 128, batch_size: int = 32):\n",
        "\n",
        "    maindf = pd.read_csv(filename)\n",
        "\n",
        "    describe_dataframe(maindf)\n",
        "\n",
        "    maindf = preprocess_dataset(maindf)\n",
        "\n",
        "    # convert an array of values into a dataset matrix\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    columns = ['index', 'Date', 'BTC_Open', 'BTC_Close', 'BTC_Volume', 'BTC_change_factor', \n",
        "    'ETH_Open', 'ETH_Close', 'ETH_Volume', 'ETH_change_factor', 'DOGE_Open', 'DOGE_Close',\n",
        "    'DOGE_Volume', 'DOGE_change_factor', 'btc_newsPosScoreaverage', 'btc_newsNegScoreaverage',\n",
        "    'btc_newsComScoreaverage', 'btc_newsScorecount', 'eth_newsPosScoreaverage', \n",
        "    'eth_newsNegScoreaverage', 'eth_newsComScoreaverage', 'eth_newsScorecount', \n",
        "    'doge_newsPosScoreaverage', 'doge_newsNegScoreaverage', 'doge_newsComScoreaverage', \n",
        "    'doge_newsScorecount', 'cryptocurrency_newsPosScoreaverage', 'cryptocurrency_newsNegScoreaverage', \n",
        "    'cryptocurrency_newsComScoreaverage', 'cryptocurrency_newsScorecount', 'economy_newsPosScoreaverage', \n",
        "    'economy_newsNegScoreaverage', 'economy_newsComScoreaverage', 'economy_newsScorecount',\n",
        "    'finance_newsPosScoreaverage', 'finance_newsNegScoreaverage', 'finance_newsComScoreaverage', \n",
        "    'finance_newsScorecount', 'politics_newsPosScoreaverage', 'politics_newsNegScoreaverage', \n",
        "    'politics_newsComScoreaverage', 'politics_newsScorecount', 'pandemic_newsPosScoreaverage', \n",
        "    'pandemic_newsNegScoreaverage', 'pandemic_newsComScoreaverage', 'pandemic_newsScorecount', \n",
        "    'btc_redditPosScoreaverage', 'btc_redditNegScoreaverage', 'btc_redditComScoreaverage', \n",
        "    'btc_redditScorecount', 'eth_redditPosScoreaverage', 'eth_redditNegScoreaverage', \n",
        "    'eth_redditComScoreaverage', 'eth_redditScorecount', 'doge_redditPosScoreaverage', \n",
        "    'doge_redditNegScoreaverage', 'doge_redditComScoreaverage', 'doge_redditScorecount', \n",
        "    'cryptocurrency_redditPosScoreaverage', 'cryptocurrency_redditNegScoreaverage', \n",
        "    'cryptocurrency_redditComScoreaverage', 'cryptocurrency_redditScorecount', \n",
        "    'economy_redditPosScoreaverage', 'economy_redditNegScoreaverage', \n",
        "    'economy_redditComScoreaverage', 'economy_redditScorecount', 'finance_redditPosScoreaverage', \n",
        "    'finance_redditNegScoreaverage', 'finance_redditComScoreaverage', 'finance_redditScorecount', \n",
        "    'politics_redditPosScoreaverage', 'politics_redditNegScoreaverage', 'politics_redditComScoreaverage', \n",
        "    'politics_redditScorecount', 'pandemic_redditPosScoreaverage', 'pandemic_redditNegScoreaverage', \n",
        "    'pandemic_redditComScoreaverage', 'pandemic_redditScorecount']\n",
        "    \"\"\"\n",
        "    columns = list(maindf.columns)[1:]\n",
        "\n",
        "    for prediction_column in prediction_columns:\n",
        "        X, y = create_dataset(maindf, columns, prediction_column, time_step)\n",
        "        print(np.shape(X), np.shape(y))\n",
        "        X_train, X_test, y_train, y_test = traintest_split(X, y)\n",
        "\n",
        "        input_shape = (None, len(X[0][0]))\n",
        "\n",
        "        # model.add(LSTM(128, \"relu\", dropout=.2, recurrent_dropout=0.2, input_shape=(None, len(X_train[0][0])))\n",
        "        # model.add(LSTM(time_step, input_dim=len(X_train[0][0]), activation=\"relu\"))\n",
        "\n",
        "        \"\"\"\n",
        "        units, activation='tanh', recurrent_activation='sigmoid', use_bias=True, \n",
        "        kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', \n",
        "        bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, \n",
        "        recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, \n",
        "        kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, \n",
        "        dropout=0.0, recurrent_dropout=0.0, return_sequences=False, return_state=False, \n",
        "        go_backwards=False, stateful=False,\n",
        "        \"\"\"\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(LSTM(num_units, dropout=0.2, recurrent_dropout=0.2, input_shape=input_shape))\n",
        "        model.add(Dense(1))\n",
        "\n",
        "        model.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[tf.keras.metrics.Accuracy()])\n",
        "        history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200, batch_size=batch_size,\n",
        "                              verbose=1)\n",
        "        \n",
        "        loss = history.history['loss']\n",
        "        val_loss = history.history['val_loss']\n",
        "\n",
        "        epochs = range(len(loss))\n",
        "\n",
        "        plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "        plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "        plt.title('Training and validation loss')\n",
        "        plt.legend(loc=0)\n",
        "        plt.figure()\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "        pad = np.zeros(time_step)\n",
        "\n",
        "        train_predict1 = model.predict(X_train).reshape(len(X_train))\n",
        "        test_predict1 = model.predict(X_test).reshape(len(X_test))\n",
        "        print(train_predict1.shape, test_predict1.shape)\n",
        "        pred = np.concatenate((pad, train_predict1, test_predict1), axis=None)\n",
        "        maindf[prediction_column+\"-prediction\"] = pd.DataFrame(pred) \n",
        "\n",
        "        maindf.to_csv(filename, index = False)\n",
        "        \n",
        "        # Evaluation metrices RMSE and MAE\n",
        "        print(\"Train data RMSE: \", math.sqrt(mean_squared_error(y_train, train_predict1)))\n",
        "        print(\"Train data MSE: \", mean_squared_error(y_train, train_predict1))\n",
        "        print(\"Train data MAE: \", mean_absolute_error(y_train, train_predict1))\n",
        "        print(\"-------------------------------------------------------------------------------------\")\n",
        "        print(\"Test data RMSE: \", math.sqrt(mean_squared_error(y_test, test_predict1)))\n",
        "        print(\"Test data MSE: \", mean_squared_error(y_test, test_predict1))\n",
        "        print(\"Test data MAE: \", mean_absolute_error(y_test, test_predict1))\n",
        "\n",
        "        \"\"\"- ## Variance Regression Score\"\"\"\n",
        "        # print(\"Train data explained variance regression score:\",\n",
        "              # explained_variance_score(y, pred[20:]))\n",
        "\n",
        "        \"\"\"- ## R square score for regression\"\"\"\n",
        "\n",
        "        print(\"Train data R2 score:\", r2_score(y_train, train_predict1))\n",
        "        print(\"Test data R2 score:\", r2_score(y_test, test_predict1))\n",
        "\n",
        "        \"\"\"- ## Regression Loss Mean Gamma deviance regression loss (MGD) and Mean Poisson deviance regression loss (MPD)\"\"\"\n",
        "\n",
        "        print(\"Train data MGD: \", mean_gamma_deviance(y_train, train_predict1))\n",
        "        print(\"Test data MGD: \", mean_gamma_deviance(y_test, test_predict1))\n",
        "        print(\"----------------------------------------------------------------------\")\n",
        "        print(\"Train data MPD: \", mean_poisson_deviance(y_train, train_predict1))\n",
        "        print(\"Test data MPD: \", mean_poisson_deviance(y_test, test_predict1))\n",
        "\n",
        "\n",
        "    return True\n",
        "def traintest_split(X: np.array, y: np.array, split: float = 0.8):\n",
        "    train_size = int(split*len(X))\n",
        "    X_train = X[:train_size]\n",
        "    X_test = X[train_size:]\n",
        "\n",
        "    y_train = y[:train_size]\n",
        "    y_test = y[train_size:]\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def deprecated(maindf: pd.DataFrame):\n",
        "    pass\n",
        "    \"\"\"\n",
        "    plt.plot(maindf['Date'][:len(y1_train)], maindf['BTC_Open'][:len(y1_train)], label = \"Actual price\")\n",
        "    plt.plot(maindf['Date'][:len(y1_train)], maindf['BTC_Open'][:len(y1_train)], label = \"Actual price\")\n",
        "    plt.title('Bitcoin Actual vs. predicted prices')\n",
        "    plt.legend(loc=0)\n",
        "    plt.figure()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    print(maindf)\n",
        "\n",
        "    maindf.dropna(subset=['BTC_pred', 'ETH_pred', 'DOGE_pred'])\n",
        "\n",
        "    maindf.to_csv(\"MPD.csv\", index=False)\n",
        "\n",
        "    - # Model Evaluation\"\"\"\n",
        "\n",
        "    # Transform back to original form\n",
        "\n",
        "    \"\"\"train_predict = scaler.inverse_transform(train_predict)\n",
        "    test_predict = scaler.inverse_transform(test_predict)\n",
        "    original_ytrain = scaler.inverse_transform(y_train.reshape(-1,1)) \n",
        "    original_ytest = scaler.inverse_transform(y_test.reshape(-1,1))\"\"\"\n",
        "\n",
        "    \"\"\"- ## Evaluation metrices RMSE, MSE and MAE\"\"\"\n",
        "\n",
        "    \"\"\"print(\"Test data explained variance regression score:\", \n",
        "          explained_variance_score(, t))\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"- # Comparision of original stock close price and predicted close price\"\"\"\n",
        "\n",
        "    \"\"\"# shift train predictions for plotting\n",
        "\n",
        "    look_back = time_step\n",
        "    trainPredictPlot = np.empty_like(closedf)\n",
        "    trainPredictPlot[:, :] = np.nan\n",
        "    trainPredictPlot[look_back:len(train_predict) + look_back, :] = train_predict\n",
        "    print(\"Train predicted data: \", trainPredictPlot.shape)\n",
        "\n",
        "    # shift test predictions for plotting\n",
        "    testPredictPlot = np.empty_like(closedf)\n",
        "    testPredictPlot[:, :] = np.nan\n",
        "    testPredictPlot[len(train_predict) + (look_back * 2) + 1:len(closedf) - 1, :] = test_predict\n",
        "    print(\"Test predicted data: \", testPredictPlot.shape)\n",
        "\n",
        "    names = cycle(['Original close price', 'Train predicted close price', 'Test predicted close price'])\n",
        "\n",
        "    plotdf = pd.DataFrame({'date': close_stock['Date'],\n",
        "                           'original_close': close_stock['BTC_Close'],\n",
        "                           'train_predicted_close': trainPredictPlot.reshape(1, -1)[0].tolist(),\n",
        "                           'test_predicted_close': testPredictPlot.reshape(1, -1)[0].tolist()})\n",
        "\n",
        "    fig = px.line(plotdf, x=plotdf['date'], y=[plotdf['original_close'], plotdf['train_predicted_close'],\n",
        "                                               plotdf['test_predicted_close']],\n",
        "                  labels={'value': 'Stock price', 'date': 'Date'})\n",
        "    fig.update_layout(title_text='Comparision between original close price vs predicted close price',\n",
        "                      plot_bgcolor='white', font_size=15, font_color='black', legend_title_text='Close Price')\n",
        "    fig.for_each_trace(lambda t: t.update(name=next(names)))\n",
        "\n",
        "    fig.update_xaxes(showgrid=False)\n",
        "    fig.update_yaxes(showgrid=False)\n",
        "    fig.show()\n",
        "    \"\"\"\n",
        "    \"\"\"- # Predicting next 30 days\"\"\"\n",
        "\n",
        "    \"\"\"    x_input = test_data[len(test_data) - time_step:].reshape(1, -1)\n",
        "    temp_input = list(x_input)\n",
        "    temp_input = temp_input[0].tolist()\n",
        "\n",
        "    from numpy import array\n",
        "\n",
        "    lst_output = []\n",
        "    n_steps = time_step\n",
        "    i = 0\n",
        "    pred_days = 30\n",
        "    while (i < pred_days):\n",
        "\n",
        "        if (len(temp_input) > time_step):\n",
        "\n",
        "            x_input = np.array(temp_input[1:])\n",
        "            # print(\"{} day input {}\".format(i,x_input))\n",
        "            x_input = x_input.reshape(1, -1)\n",
        "            x_input = x_input.reshape((1, n_steps, 1))\n",
        "\n",
        "            yhat = model.predict(x_input, verbose=0)\n",
        "            # print(\"{} day output {}\".format(i,yhat))\n",
        "            temp_input.extend(yhat[0].tolist())\n",
        "            temp_input = temp_input[1:]\n",
        "            # print(temp_input)\n",
        "\n",
        "            lst_output.extend(yhat.tolist())\n",
        "            i = i + 1\n",
        "\n",
        "        else:\n",
        "\n",
        "            x_input = x_input.reshape((1, n_steps, 1))\n",
        "            yhat = model.predict(x_input, verbose=0)\n",
        "            temp_input.extend(yhat[0].tolist())\n",
        "\n",
        "            lst_output.extend(yhat.tolist())\n",
        "            i = i + 1\n",
        "\n",
        "    print(\"Output of predicted next days: \", len(lst_output))\n",
        "    \"\"\"\n",
        "    \"\"\"- # Plotting last 15 days of dataset and next predicted 30 days\"\"\"\n",
        "\n",
        "    \"\"\"    last_days = np.arange(1, time_step + 1)\n",
        "    day_pred = np.arange(time_step + 1, time_step + pred_days + 1)\n",
        "    print(last_days)\n",
        "    print(day_pred)\n",
        "\n",
        "    temp_mat = np.empty((len(last_days) + pred_days + 1, 1))\n",
        "    temp_mat[:] = np.nan\n",
        "    temp_mat = temp_mat.reshape(1, -1).tolist()[0]\n",
        "\n",
        "    last_original_days_value = temp_mat\n",
        "    next_predicted_days_value = temp_mat\n",
        "\n",
        "    last_original_days_value[0:time_step + 1] = \\\n",
        "    scaler.inverse_transform(closedf[len(closedf) - time_step:]).reshape(1, -1).tolist()[0]\n",
        "    next_predicted_days_value[time_step + 1:] = \\\n",
        "    scaler.inverse_transform(np.array(lst_output).reshape(-1, 1)).reshape(1, -1).tolist()[0]\n",
        "\n",
        "    new_pred_plot = pd.DataFrame({\n",
        "        'last_original_days_value': last_original_days_value,\n",
        "        'next_predicted_days_value': next_predicted_days_value\n",
        "    })\n",
        "\n",
        "    names = cycle(['Last 15 days close price', 'Predicted next 30 days close price'])\n",
        "\n",
        "    fig = px.line(new_pred_plot, x=new_pred_plot.index, y=[new_pred_plot['last_original_days_value'],\n",
        "                                                           new_pred_plot['next_predicted_days_value']],\n",
        "                  labels={'value': 'Stock price', 'index': 'Timestamp'})\n",
        "    fig.update_layout(title_text='Compare last 15 days vs next 30 days',\n",
        "                      plot_bgcolor='white', font_size=15, font_color='black', legend_title_text='Close Price')\n",
        "\n",
        "    fig.for_each_trace(lambda t: t.update(name=next(names)))\n",
        "    fig.update_xaxes(showgrid=False)\n",
        "    fig.update_yaxes(showgrid=False)\n",
        "    fig.show()\n",
        "    \"\"\"\n",
        "    # Plotting entire Closing Stock Price with next 30 days period of prediction\"\"\"\n",
        "    \"\"\"\n",
        "    lstmdf = closedf.tolist()\n",
        "    lstmdf.extend((np.array(lst_output).reshape(-1, 1)).tolist())\n",
        "    lstmdf = scaler.inverse_transform(lstmdf).reshape(1, -1).tolist()[0]\n",
        "\n",
        "    names = cycle(['Close price'])\n",
        "\n",
        "    fig = px.line(lstmdf, labels={'value': 'Stock price', 'index': 'Timestamp'})\n",
        "    fig.update_layout(title_text='Plotting whole closing stock price with prediction',\n",
        "                      plot_bgcolor='white', font_size=15, font_color='black', legend_title_text='Stock')\n",
        "\n",
        "    fig.for_each_trace(lambda t: t.update(name=next(names)))\n",
        "\n",
        "    fig.update_xaxes(showgrid=False)\n",
        "    fig.update_yaxes(showgrid=False)\n",
        "    fig.show()\n",
        "    \"\"\"\n",
        "    \"\"\"Thats it we are Done with Bitcoin Price Prediction using LSTM.\"\"\"\n",
        "\n",
        "def preprocess_dataset(df):\n",
        "    df.replace(to_replace=np.nan, value=0.0, inplace=True)\n",
        "    print(df.shape)\n",
        "    # normalize_dataset(df)\n",
        "    return df\n",
        "\n",
        "def create_dataset(dataset: pd.DataFrame, columns_X: list, column_Y: str, time_step=1):\n",
        "\n",
        "    # create x and y datasets, to be passed into model\n",
        "    # \n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(dataset) - time_step - 1):\n",
        "        # The daily subset ranges from the index up until index + timestep -1\n",
        "          # Becaus\n",
        "        daily_subset = dataset.loc[i:(i + time_step - 1), columns_X]\n",
        "        dataX.append(daily_subset)\n",
        "        dataY.append(dataset.loc[i + time_step, column_Y])\n",
        "    return np.array(dataX), np.array(dataY)\n",
        "\n",
        "def normalize_dataset(data: pd.DataFrame):\n",
        "    columns = list(data.columns)\n",
        "    for column in columns[1:]:\n",
        "        data[column] = normalize_column(data[column])\n",
        "\n",
        "def normalize_column(column: pd.Series) -> pd.Series:\n",
        "    STD = np.std(column)\n",
        "    mean = np.mean(column)\n",
        "    column = column.apply(lambda x: (x - mean) / STD)\n",
        "    return column\n",
        "\n",
        "def describe_dataframe(df: pd.DataFrame):\n",
        "    print(df)\n",
        "\n",
        "    print('Total number of days present in the dataset: ', df.shape[0])\n",
        "    print('Total number of fields present in the dataset: ', df.shape[1])\n",
        "\n",
        "    print(df.shape,\n",
        "\n",
        "    df.head(),\n",
        "\n",
        "    df.tail(),\n",
        "\n",
        "    df.info(),\n",
        "\n",
        "    df.describe(),\n",
        "\n",
        "    df.columns)\n",
        "\n",
        "    print('\\n\\n\\nNull Values:', df.isnull().values.sum())\n",
        "\n",
        "    print('\\n\\n\\nNA values:', df.isnull().values.any(), \"\\n\\n\\n\")\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "\n",
        "    # script, filepath, currencies = sys.argv\n",
        "    # currencies = currencies.split()\n",
        "    filename = '/content/MyDrive/MyDrive/ISEF/Datasets/MergedPriceDataEMA7.csv'\n",
        "    filepath = '/content/MyDrive/MyDrive/ISEF/Datasets'\n",
        "    # drive.mount('/content/MyDrive')\n",
        "\n",
        "    # filepath = './'\n",
        "    # filename = './MergedPriceDataEMA.csv'\n",
        "    prediction_columns = [\n",
        "        'BTC-ChangeFactor-EMA7',\n",
        "        'ETH-ChangeFactor-EMA7',\n",
        "        'DOGE-ChangeFactor-EMA7',\n",
        "        'BTC-ChangeFactor',\n",
        "        'ETH-ChangeFactor',\n",
        "        'DOGE-ChangeFactor'\n",
        "    ]\n",
        "\n",
        "    main(filepath, filename, prediction_columns, time_step = 15)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = [1, 2, 3, 4, 5]\n",
        "a[1:3]"
      ],
      "metadata": {
        "id": "cGPomIWaJTk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SKvVUUnLj6iu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "LSTM_Updated.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN3NqCeUxS8TXykhMTDyMG9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}